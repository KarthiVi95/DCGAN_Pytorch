{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below was run on a GPU enabled server to produce results.\n",
    "\n",
    "\n",
    "refernce : https://github.com/pytorch/examples/tree/master/dcgan\n",
    "Official Pytorch Github page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Oct 20 17:33:41 2019\n",
    "\n",
    "@author: Karthik Vikram\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import math\n",
    "import warnings\n",
    "import numpy as np\n",
    "from scipy import linalg\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "#from tqdm import tqdm_notebook as tqdm\n",
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataroot', required=True)\n",
    "parser.add_argument('--workers', type=int, help='number of data loading workers', default=2)\n",
    "parser.add_argument('--batchSize', type=int, default=128, help='input batch size')\n",
    "parser.add_argument('--imageSize', type=int, default=32, help='the height / width of the input image to network')\n",
    "parser.add_argument('--nz', type=int, default=100, help='size of the latent z vector')\n",
    "parser.add_argument('--ngf', type=int, default=64)\n",
    "parser.add_argument('--ndf', type=int, default=64)\n",
    "parser.add_argument('--niter', type=int, default=120, help='number of epochs to train for')\n",
    "parser.add_argument('--lr', type=float, default=0.0005, help='learning rate, default=0.0002')\n",
    "parser.add_argument('--beta1', type=float, default=0.5, help='beta1 for adam. default=0.5')\n",
    "parser.add_argument('--cuda', action='store_true', help='enables cuda')\n",
    "parser.add_argument('--ngpu', type=int, default=1, help='number of GPUs to use')\n",
    "parser.add_argument('--netG', default='', help=\"path to netG (to continue training)\")\n",
    "parser.add_argument('--netD', default='', help=\"path to netD (to continue training)\")\n",
    "parser.add_argument('--outf', default='.', help='folder to output images and model checkpoints')\n",
    "parser.add_argument('--manualSeed', type=int, help='manual seed')\n",
    "parser.add_argument('--classes', default='bedroom', help='comma separated list of classes for the lsun data set')\n",
    "\n",
    "opt = parser.parse_args()\n",
    "\n",
    "try:\n",
    "    os.makedirs(opt.outf)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "def setManualSeed(opt):\n",
    "    # Create manual seed for the seeding\n",
    "    if opt.manualSeed is None:\n",
    "        opt.manualSeed = random.randint(1, 10000)\n",
    "    print(\"Random Seed: \", opt.manualSeed)\n",
    "    random.seed(opt.manualSeed)\n",
    "    torch.manual_seed(opt.manualSeed)\n",
    "\n",
    "setManualSeed(opt)\n",
    "\n",
    "cudnn.benchmark = True\n",
    "\n",
    "# Number of channels in the data (R,G,B) for the Cifar10 dataset\n",
    "nc=3\n",
    "\n",
    "def setData():\n",
    "    # Data Preprocessing. Normalizing the pixels and tensors\n",
    "    transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "    trainset = dset.CIFAR10(root=opt.dataroot, train=True,\n",
    "                                            download=True, transform=transform)\n",
    "    assert trainset\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=opt.batchSize,\n",
    "                                              shuffle=True, num_workers=int(opt.workers))\n",
    "    \n",
    "    testset = dset.CIFAR10(root=opt.dataroot, train=False,\n",
    "                                           download=True, transform=transform)\n",
    "    assert testset\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=opt.batchSize,\n",
    "                                             shuffle=False, num_workers=int(opt.workers))\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(\"cifar10\", batch_size=opt.batchSize,\n",
    "                                             shuffle=True, num_workers=int(opt.workers))\n",
    "    return (trainset,trainloader,testset,testloader,dataloader)\n",
    "\n",
    "# data iterators set for Cifar10 dataset\n",
    "trainset,trainloader,testset,testloader,dataloader=setData()\n",
    "\n",
    "# Setting the first GPU for usage\n",
    "device = torch.device(\"cuda:0\")\n",
    "ngpu = int(opt.ngpu)\n",
    "nz = int(opt.nz)\n",
    "ngf = int(opt.ngf)\n",
    "ndf = int(opt.ndf)\n",
    "\n",
    "\n",
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            # batch normalizing the convolution output\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            # adding relu activation to the layer\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d(ngf * 2, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        return output\n",
    "\n",
    "\n",
    "netG = Generator(ngpu).to(device)\n",
    "# Passing the weights initialization function for the Generator\n",
    "\n",
    "netG.apply(weights_init)\n",
    "if opt.netG != '':\n",
    "    loaded_model=torch.load(opt.netD)\n",
    "    netG.load_state_dict(loaded_model[\"gen_state_dict\"])\n",
    "print(netG)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(nc, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        return output.view(-1, 1).squeeze(1)\n",
    "\n",
    "# Assigning the discriminator model to the GPU\n",
    "netD = Discriminator(ngpu).to(device)\n",
    "\n",
    "# Applying weight initialization to the discriminator\n",
    "netD.apply(weights_init)\n",
    "if opt.netD != '':\n",
    "    loaded_model=torch.load(opt.netD)\n",
    "    netD.load_state_dict(loaded_model[\"disc_state_dict\"])\n",
    "print(netD)\n",
    "\n",
    "# Setting binary cross entropy loss to the model\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "fixed_noise = torch.randn(opt.batchSize, nz, 1, 1, device=device)\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "# setup optimizer\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n",
    "# Following the two scale update rule and setting the learning rate of generator\n",
    "# twice that of the discriminator\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=opt.lr*2.0, betas=(opt.beta1, 0.999))\n",
    "\n",
    "class FID():\n",
    "    '''\n",
    "    Code for FID Calculation taken from TA's piazza post\n",
    "    '''\n",
    "    def __init__(self, cache_dir='./Cache', device='cpu', transform_input=True):\n",
    "        os.environ[\"TORCH_HOME\"] = \"./Cache\"\n",
    "        self.device=device\n",
    "        self.transform_input = transform_input\n",
    "        self.InceptionV3 = models.inception_v3(pretrained=True, transform_input=False, aux_logits=False).to(device=self.device)\n",
    "        self.InceptionV3.eval()\n",
    "    \n",
    "    def build_maps(self, x):\n",
    "        # Resize to Fit InceptionV3\n",
    "        if list(x.shape[-2:]) != [299,299]:\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "                x = F.interpolate(x, size=[299,299], mode='bilinear')\n",
    "        # Transform Input to InceptionV3 Standards\n",
    "        if self.transform_input:\n",
    "            x_ch0 = torch.unsqueeze(x[:, 0], 1) * (0.229 / 0.5) + (0.485 - 0.5) / 0.5\n",
    "            x_ch1 = torch.unsqueeze(x[:, 1], 1) * (0.224 / 0.5) + (0.456 - 0.5) / 0.5\n",
    "            x_ch2 = torch.unsqueeze(x[:, 2], 1) * (0.225 / 0.5) + (0.406 - 0.5) / 0.5\n",
    "            x = torch.cat((x_ch0, x_ch1, x_ch2), 1)\n",
    "        # Run Through Partial InceptionV3 Model\n",
    "        with torch.no_grad():\n",
    "            # N x 3 x 299 x 299\n",
    "            x = self.InceptionV3.Conv2d_1a_3x3(x)\n",
    "            # N x 32 x 149 x 149\n",
    "            x = self.InceptionV3.Conv2d_2a_3x3(x)\n",
    "            # N x 32 x 147 x 147\n",
    "            x = self.InceptionV3.Conv2d_2b_3x3(x)\n",
    "            # N x 64 x 147 x 147\n",
    "            x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
    "            # N x 64 x 73 x 73\n",
    "            x = self.InceptionV3.Conv2d_3b_1x1(x)\n",
    "            # N x 80 x 73 x 73\n",
    "            x = self.InceptionV3.Conv2d_4a_3x3(x)\n",
    "            # N x 192 x 71 x 71\n",
    "            x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
    "            # N x 192 x 35 x 35\n",
    "            x = self.InceptionV3.Mixed_5b(x)\n",
    "            # N x 256 x 35 x 35\n",
    "            x = self.InceptionV3.Mixed_5c(x)\n",
    "            # N x 288 x 35 x 35\n",
    "            x = self.InceptionV3.Mixed_5d(x)\n",
    "            # N x 288 x 35 x 35\n",
    "            x = self.InceptionV3.Mixed_6a(x)\n",
    "            # N x 768 x 17 x 17\n",
    "            x = self.InceptionV3.Mixed_6b(x)\n",
    "            # N x 768 x 17 x 17\n",
    "            x = self.InceptionV3.Mixed_6c(x)\n",
    "            # N x 768 x 17 x 17\n",
    "            x = self.InceptionV3.Mixed_6d(x)\n",
    "            # N x 768 x 17 x 17\n",
    "            x = self.InceptionV3.Mixed_6e(x)\n",
    "            # N x 768 x 17 x 17\n",
    "            x = self.InceptionV3.Mixed_7a(x)\n",
    "            # N x 1280 x 8 x 8\n",
    "            x = self.InceptionV3.Mixed_7b(x)\n",
    "            # N x 2048 x 8 x 8\n",
    "            x = self.InceptionV3.Mixed_7c(x)\n",
    "            # N x 2048 x 8 x 8\n",
    "            # Adaptive average pooling\n",
    "            x = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "            # N x 2048 x 1 x 1\n",
    "            return x\n",
    "    \n",
    "    def compute_fid(self, real_images, generated_images, batch_size=64):\n",
    "        # Ensure Set Sizes are the Same\n",
    "        assert(real_images.shape[0] == generated_images.shape[0])\n",
    "        # Build Random Sampling Orders\n",
    "        real_images = real_images[np.random.permutation(real_images.shape[0])]\n",
    "        generated_images = generated_images[np.random.permutation(generated_images.shape[0])]\n",
    "        # Lists of Maps per Batch\n",
    "        real_maps = []\n",
    "        generated_maps = []\n",
    "        # Build Maps\n",
    "#        for s in tqdm(range(int(math.ceil(real_images.shape[0]/batch_size))), desc='Evaluation', leave=False):\n",
    "        for s in range(int(math.ceil(real_images.shape[0]/batch_size))):\n",
    "            sidx = np.arange(batch_size*s, min(batch_size*(s+1), real_images.shape[0]))\n",
    "            real_maps.append(self.build_maps(real_images[sidx].to(device=self.device)).detach().to(device='cpu'))\n",
    "#            real_maps.append(self.build_maps(real_images[sidx]).detach())\n",
    "            generated_maps.append(self.build_maps(generated_images[sidx].to(device=self.device)).detach().to(device='cpu'))\n",
    "#            generated_maps.append(self.build_maps(generated_images[sidx]).detach())\n",
    "        # Concatenate Maps\n",
    "        real_maps = np.squeeze(torch.cat(real_maps).numpy())\n",
    "        generated_maps = np.squeeze(torch.cat(generated_maps).numpy())\n",
    "        # Calculate FID\n",
    "        # Activation Statistics\n",
    "        mu_g = np.mean(generated_maps, axis=0)\n",
    "        mu_x = np.mean(real_maps, axis=0)\n",
    "        sigma_g = np.cov(generated_maps, rowvar=False)\n",
    "        sigma_x = np.cov(real_maps, rowvar=False)\n",
    "        # Sum of Squared Differences\n",
    "        ssd = np.sum((mu_g - mu_x)**2)\n",
    "        # Square Root of Product of Covariances\n",
    "        covmean = linalg.sqrtm(sigma_g.dot(sigma_x), disp=False)[0]\n",
    "        if np.iscomplexobj(covmean):\n",
    "            covmean = covmean.real\n",
    "        # Final FID Computation\n",
    "        return ssd + np.trace(sigma_g + sigma_x - 2*covmean)\n",
    "\n",
    "fid_obj = FID()\n",
    "\n",
    "#start = time.time()\n",
    "writer = ()\n",
    "for epoch in range(opt.niter): #for each epoch\n",
    "    start = time.time()\n",
    "    for i, data in enumerate(trainloader, 0): #for each batch of trainset using trainloader\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        # train discriminator with real\n",
    "        netD.zero_grad() #clear grad data for this iteration\n",
    "        real_cpu = data[0].to(device)\n",
    "        batch_size = real_cpu.size(0)\n",
    "        label = torch.full((batch_size,), real_label, device=device) #label 1\n",
    "\n",
    "        output_D = netD(real_cpu) #get discriminator prediction\n",
    "        errD_real = criterion(output_D, label) #apply BCE loss function on output with real label values\n",
    "        errD_real.backward() #back propagate the loss in discriminator \n",
    "        D_x = output_D.mean().item() #final discriminator loss on real data\n",
    "\n",
    "        # train with fake\n",
    "        noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "        fake = netG(noise) #generate noise data\n",
    "        label.fill_(fake_label) \n",
    "        output = netD(fake.detach()) #get discriminator prediction\n",
    "        errD_fake = criterion(output, label) #apply BCE loss function\n",
    "        errD_fake.backward() #back propagate the loss in discriminator\n",
    "        D_G_z1 = output.mean().item() #final discriminator loss on fake data\n",
    "        errD = errD_real + errD_fake #final discriminator error\n",
    "        optimizerD.step() #apply Adam optimizer\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        output = netD(fake) #prediction of discriminator\n",
    "        errG = criterion(output, label) #generator loss function\n",
    "        errG.backward() #back propagate in generator\n",
    "        D_G_z2 = output.mean().item() \n",
    "        optimizerG.step() #use adam optimizer\n",
    "\n",
    "        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f'\n",
    "              % (epoch, opt.niter, i, len(trainloader),\n",
    "                 errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "                \n",
    "        #end of epoch operations\n",
    "        if i == len(trainloader)-1: #if last iteration in epoch\n",
    "            vutils.save_image(real_cpu, #save image of real sample\n",
    "                    '%s/real_samples.png' % opt.outf,\n",
    "                    normalize=True)\n",
    "            fake = netG(fixed_noise) #generate fake samples using generator\n",
    "            vutils.save_image(fake.detach(), #save fake images generated\n",
    "                    '%s/fake_samples_epoch_%03d.png' % (opt.outf, epoch),\n",
    "                    normalize=True)\n",
    "\n",
    "            #find FID\n",
    "            # Number of images to compare for calculating FID score\n",
    "            comparison_size = 1000\n",
    "            #fake images for FID score\n",
    "            noise = torch.randn(comparison_size, nz, 1, 1, device=device)\n",
    "            fake_data = netG(noise)\n",
    "\n",
    "            #real images picked randomly from test data for FID score\n",
    "            real_data = None\n",
    "            rand_sampler = torch.utils.data.RandomSampler(testset, num_samples=comparison_size, replacement=True)\n",
    "            test_sampler = torch.utils.data.DataLoader(testset, batch_size=comparison_size, sampler=rand_sampler)\n",
    "            for i,data in enumerate(test_sampler, 0):\n",
    "              real_data = data[0]\n",
    "              break\n",
    "            #computing FID score\n",
    "            fid_val = fid_obj.compute_fid(real_data, fake_data)\n",
    "            print(fid_val)\n",
    "\n",
    "            if epoch % 10 == 0: #save generator and discriminator states once every 50 epochs\n",
    "              # do checkpointing\n",
    "              torch.save(netG.state_dict(), '%s/netG_epoch_%d.pth' % (opt.outf, epoch))\n",
    "              torch.save(netD.state_dict(), '%s/netD_epoch_%d.pth' % (opt.outf, epoch))\n",
    "\n",
    "            writer.add_scalar('Loss/real_discriminator', errD_real.item(), epoch)\n",
    "            writer.add_scalar('Loss/fake_discriminator', errD_fake.item(), epoch)\n",
    "            writer.add_scalar('Loss/discriminator', errD.item(), epoch)\n",
    "            writer.add_scalar('Loss/generator', errG.item(), epoch)\n",
    "            writer.add_scalar('FID', fid_val, epoch)\n",
    "\n",
    "    print('Time per epoch: ', time.time()-start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to see the output for DCGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from _future_ import print_function\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "batchSize = 64\n",
    "imageSize = 32 #image resolution\n",
    "nz = 100  #noise vector size\n",
    "ngf = 64  #base number of generator filters\n",
    "ndf = 64 #base number of descriminator filters\n",
    "niter = 1 # number of epochs #TODO: change to 100000\n",
    "lr = 0.0002 #learning rate\n",
    "cuda = True #for parallel execution in GPU\n",
    "ngpu = 1 #number of GPUs used\n",
    "outf = '.' #output file path\n",
    "manualseed = 350 #random seed value\n",
    "nc = 3\n",
    "device = torch.device(\"cuda:0\" if cuda else \"cpu\")\n",
    "\n",
    "#generator model definition\n",
    "class Generator(nn.Module):\n",
    "    def _init_(self, ngpu):\n",
    "        super(Generator, self)._init_()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf * 2, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        ) \n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "\n",
    "netG = Generator(ngpu).to(device) #initialize generator\n",
    "\n",
    "netG.load_state_dict(torch.load('./DCGAN.pth'))\n",
    "noise = torch.randn(64, nz, 1, 1, device=device)#generate noise data\n",
    "fake = netG(noise) #generated fake images\n",
    "\n",
    "vutils.save_image(fake.detach(),'./image_final.png',normalize=True)\n",
    "\n",
    "img = mpimg.imread('image_final.png')\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
